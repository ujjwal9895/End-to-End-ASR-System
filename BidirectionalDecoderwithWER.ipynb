{
 "metadata": {
  "name": "",
  "signature": "sha256:caa53ee72ddf1feb4c1d8c6c8a1664ed9773fb016b3c80d673eacdf835a14970"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tensorflow as tf\n",
      "import pickle\n",
      "import numpy as np\n",
      "from tensorflow.contrib.legacy_seq2seq.python.ops import seq2seq\n",
      "from tensorflow.python.ops import ctc_ops as ctc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_mfcc = []\n",
      "train_words = []\n",
      "test_mfcc = []\n",
      "test_words = []\n",
      "vocab_id_2_c = {}\n",
      "vocab_c_2_id = {}\n",
      "x = \"PGabcdefghijklmnopqrstuvwxyz0123456789 ,.'?-UE\"\n",
      "\n",
      "with open(\"train_mfcc\", \"rb\") as f:\n",
      "    train_mfcc = pickle.load(f)\n",
      "    \n",
      "with open(\"train_output_words\", \"rb\") as f:\n",
      "    train_words = pickle.load(f)\n",
      "    \n",
      "with open(\"test_mfcc\", \"rb\") as f:\n",
      "    test_mfcc = pickle.load(f)\n",
      "    \n",
      "with open(\"test_output_words\", \"rb\") as f:\n",
      "    test_words = pickle.load(f)\n",
      "    \n",
      "for i in range(len(x)):\n",
      "    vocab_c_2_id[x[i]] = i + 1\n",
      "    vocab_id_2_c[i + 1] = x[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_targets = []\n",
      "\n",
      "with open(\"train_words_text\", 'r') as f:\n",
      "\n",
      "    for line in f.readlines():\n",
      "\n",
      "        original = ' '.join(line.strip().lower().split(' ')).replace('.', '')\n",
      "        targets = original.replace(' ', '  ')\n",
      "        targets = targets.split(' ')\n",
      "\n",
      "        targets = np.hstack([list(x) for x in targets])\n",
      "\n",
      "        targets = np.asarray([vocab_c_2_id[x] if x in vocab_c_2_id else vocab_c_2_id['U']\n",
      "                              for x in targets])\n",
      "    \n",
      "        train_targets.append(targets)\n",
      "        \n",
      "test_targets = []\n",
      "\n",
      "with open(\"test_words_text\", \"r\") as f:\n",
      "    \n",
      "    for line in f.readlines():\n",
      "        original = ' '.join(line.strip().lower().split(' ')).replace('.', '')\n",
      "        targets = original.replace(' ', '  ')\n",
      "        targets = targets.split(' ')\n",
      "\n",
      "        targets = np.hstack([list(x) for x in targets])\n",
      "\n",
      "        targets = np.asarray([vocab_c_2_id[x] if x in vocab_c_2_id else vocab_c_2_id['U']\n",
      "                              for x in targets])\n",
      "    \n",
      "        test_targets.append(targets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_hidden = 128\n",
      "num_classes = len(vocab_c_2_id) + 1\n",
      "batch_size = 1\n",
      "input_size = 123\n",
      "max_encoder_input_size = 0\n",
      "max_decoder_input_size = 0\n",
      "train_ids = []\n",
      "\n",
      "for i in range(len(train_mfcc)):\n",
      "    max_encoder_input_size = max(max_encoder_input_size, len(train_mfcc[i]))\n",
      "    \n",
      "for i in range(len(test_mfcc)):\n",
      "    max_encoder_input_size = max(max_encoder_input_size, len(test_mfcc[i]))\n",
      "    \n",
      "# for i in range(len(train_words)):\n",
      "#     train_words[i] = train_words[i].lower()\n",
      "#     max_decoder_input_size = max(max_decoder_input_size, len(train_words[i]))\n",
      "    \n",
      "for i in range(len(train_mfcc)):\n",
      "    while train_mfcc[i].shape[0] < max_encoder_input_size:\n",
      "        train_mfcc[i] = np.vstack([train_mfcc[i], [0] * 123])\n",
      "        \n",
      "for i in range(len(test_mfcc)):\n",
      "    while test_mfcc[i].shape[0] < max_encoder_input_size:\n",
      "        test_mfcc[i] = np.vstack([test_mfcc[i], [0] * 123])\n",
      "        \n",
      "# for i in range(len(train_words)):\n",
      "#     data = []\n",
      "#     data2 = []\n",
      "#     for j in range(max_decoder_input_size):\n",
      "        \n",
      "#         if j < len(train_words[i]):\n",
      "#         if j == 0:\n",
      "#             data = (np.arange(num_classes) == vocab_c_2_id['G']).astype(np.float32)\n",
      "#             if train_words[i][j] in vocab_c_2_id:\n",
      "#                 data2 = (np.arange(num_classes) == vocab_c_2_id[train_words[i][j]]).astype(np.float32)\n",
      "#             else:\n",
      "#                 data2 = (np.arange(num_classes) == vocab_c_2_id['U']).astype(np.float32)\n",
      "#         elif j < len(train_words[i]):\n",
      "#             if train_words[i][j-1] in vocab_c_2_id:\n",
      "#                 data = np.vstack([data, (np.arange(num_classes) == vocab_c_2_id[train_words[i][j-1]]).astype(np.float32)])\n",
      "#             else:\n",
      "#                 data = np.vstack([data, (np.arange(num_classes) == vocab_c_2_id['U']).astype(np.float32)])\n",
      "#             if train_words[i][j] in vocab_c_2_id:\n",
      "#                 data2 = np.vstack([data2, (np.arange(num_classes) == vocab_c_2_id[train_words[i][j]]).astype(np.float32)])\n",
      "#             else:\n",
      "#                 data2 = np.vstack([data2, (np.arange(num_classes) == vocab_c_2_id['U']).astype(np.float32)])\n",
      "#         elif j == len(train_words[i]):\n",
      "#             if train_words[i][j-1] in vocab_c_2_id:\n",
      "#                 data = np.vstack([data, (np.arange(num_classes) == vocab_c_2_id[train_words[i][j-1]]).astype(np.float32)])\n",
      "#             else:\n",
      "#                 data = np.vstack([data, (np.arange(num_classes) == vocab_c_2_id['U']).astype(np.float32)])\n",
      "#             data2 = np.vstack([data2, [0] * num_classes]) \n",
      "#         else:\n",
      "#             data = np.vstack([data, [0] * num_classes])\n",
      "#             data2 = np.vstack([data2, [0] * num_classes])\n",
      "#     train_ids.append(data)\n",
      "#     train_targets.append(data2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in range(len(train_targets)):\n",
      "    max_decoder_input_size = max(max_decoder_input_size, len(train_targets[i]))\n",
      "    \n",
      "for i in range(len(test_targets)):\n",
      "    max_decoder_input_size = max(max_decoder_input_size, len(test_targets[i]))\n",
      "\n",
      "for i in range(len(train_targets)):\n",
      "    while len(train_targets[i]) < max_decoder_input_size:\n",
      "        train_targets[i] = np.append(train_targets[i], vocab_c_2_id['P'])\n",
      "        \n",
      "for i in range(len(test_targets)):\n",
      "    while len(test_targets[i]) < max_decoder_input_size:\n",
      "        test_targets[i] = np.append(test_targets[i], vocab_c_2_id['P'])\n",
      "        \n",
      "        \n",
      "train_ts = []\n",
      "for i in range(len(train_targets)):\n",
      "    data = []\n",
      "    for j in range(len(train_targets[i])):\n",
      "        if j == 0:\n",
      "            data = (np.arange(num_classes) == train_targets[i][j]).astype(np.float32)\n",
      "        else:\n",
      "            data = np.vstack([data, (np.arange(num_classes) == train_targets[i][j]).astype(np.float32)])\n",
      "    train_ts.append(data)\n",
      "    \n",
      "test_ts = []\n",
      "for i in range(len(test_targets)):\n",
      "    data = []\n",
      "    for j in range(len(test_targets[i])):\n",
      "        if j == 0:\n",
      "            data = (np.arange(num_classes) == test_targets[i][j]).astype(np.float32)\n",
      "        else:\n",
      "            data = np.vstack([data, (np.arange(num_classes) == test_targets[i][j]).astype(np.float32)])\n",
      "    test_ts.append(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "weights_attend = tf.Variable(tf.truncated_normal([2 * num_hidden, num_hidden],\n",
      "                                                   stddev=np.sqrt(2.0 / (2*num_hidden))))\n",
      "\n",
      "biases_attend = tf.Variable(tf.zeros([num_hidden]))\n",
      "\n",
      "# weightsClasses = tf.Variable(tf.truncated_normal([num_hidden, num_classes],\n",
      "#                                                      stddev=np.sqrt(2.0 / num_hidden)))\n",
      "# biasesClasses = tf.Variable(tf.zeros([num_classes]))\n",
      "\n",
      "\n",
      "inputs = tf.placeholder(tf.float32, shape = (max_encoder_input_size, batch_size, input_size))\n",
      "inputrs = tf.reshape(inputs, [-1, input_size])\n",
      "\n",
      "inputList = tf.split(inputrs, max_encoder_input_size, 0)\n",
      "\n",
      "tar = tf.placeholder(tf.float32, shape = (max_decoder_input_size, batch_size, num_classes))\n",
      "\n",
      "tars = []\n",
      "for j in range(max_decoder_input_size):\n",
      "    tars.append(tar[j])\n",
      "\n",
      "fw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple = True)\n",
      "bw = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple = True)\n",
      "\n",
      "output, _, _ = tf.contrib.rnn.static_bidirectional_rnn(fw, bw, inputList, dtype = tf.float32)\n",
      "\n",
      "# outH1 = [tf.reduce_sum(tf.multiply(t, weightsOutH1), reduction_indices = 1) + biasesOutH1 for t in outputrs]\n",
      "\n",
      "decoder_attention_states = [tf.matmul(t, weights_attend) + biases_attend for t in output]\n",
      "decoder_attention_states = tf.reshape(decoder_attention_states, shape = (batch_size, max_encoder_input_size, num_hidden))\n",
      "\n",
      "decoder_cell = tf.contrib.rnn.LSTMCell(num_hidden)\n",
      "decoder_cell_state = decoder_cell.zero_state(batch_size, dtype = tf.float32)\n",
      "\n",
      "output_decoder, decoder_cell_state = tf.contrib.legacy_seq2seq.attention_decoder(tars, decoder_cell_state, \n",
      "                                        decoder_attention_states, decoder_cell)\n",
      "\n",
      "weights_decoder = tf.Variable(tf.truncated_normal([num_hidden, num_classes],\n",
      "                                                   stddev=np.sqrt(2.0 / (2*num_hidden))))\n",
      "biases_decoder = tf.Variable(tf.zeros([num_classes]))\n",
      "\n",
      "output_decoder = tf.reshape(output_decoder, shape = (max_decoder_input_size, num_hidden))\n",
      "output_logits = tf.matmul(output_decoder, weights_decoder) + biases_decoder\n",
      "\n",
      "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = output_logits, labels = tar))\n",
      "\n",
      "optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predictions = tf.argmax(output_logits, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def accuracy(preds, ls):\n",
      "  return (100.0 * np.sum(preds == np.argmax(ls, 1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 1\n",
      "\n",
      "with tf.Session() as session:\n",
      "    \n",
      "    print(\"Initializing\")\n",
      "    tf.global_variables_initializer().run()\n",
      "    \n",
      "    saver = tf.train.Saver()\n",
      "    saver.restore(session, \"./Models/bidirectionaldecoder.ckpt\")\n",
      "    \n",
      "    for step in range(num_steps):\n",
      "        \n",
      "        l2 = 0\n",
      "        for i in range(len(test_mfcc)):\n",
      "            \n",
      "            feedDict = {inputs : test_mfcc[i].reshape([max_encoder_input_size, batch_size, input_size]),\n",
      "                        tar : test_ts[i].reshape([max_decoder_input_size, batch_size, num_classes])}\n",
      "            \n",
      "            \n",
      "            pred = session.run(predictions, feed_dict=feedDict)\n",
      "            \n",
      "            l = 100.0 * np.sum(pred == np.argmax(test_ts[i], 1)) / len(test_ts[i])\n",
      "            \n",
      "            l2 += l\n",
      "            if i % 500 == 0:\n",
      "                print(\"I - test\", i, l2 / (i + 1))\n",
      "                    \n",
      "        l2 /= len(test_mfcc)\n",
      "        print(\"Test Loss\", l2)\n",
      "        \n",
      "#         l2 = 0\n",
      "#         for i in range(len(train_mfcc)):\n",
      "            \n",
      "#             feedDict = {inputs : train_mfcc[i].reshape([max_encoder_input_size, batch_size, input_size]),\n",
      "#                         tar : train_ts[i].reshape([max_decoder_input_size, batch_size, num_classes])}\n",
      "            \n",
      "            \n",
      "#             _, l = session.run([optimizer, loss], feed_dict=feedDict)\n",
      "            \n",
      "#             l2 += l\n",
      "            \n",
      "#             if i % 500 == 0:\n",
      "#                 print(\"I\", i, l2)\n",
      "                    \n",
      "#         print(\"Step\", step, \"Loss\", l2)\n",
      "#         saver.save(session, \"./Models/bidirectionaldecoder.ckpt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initializing\n",
        "('I - test', 0, 65.71428571428571)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('I - test', 500, 49.85742800114058)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('I - test', 1000, 49.99714571143144)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('I - test', 1500, 50.098029884838716)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Test Loss', 50.1352040816326)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 1\n",
      "\n",
      "with tf.Session() as session:\n",
      "    \n",
      "    print(\"Initializing\")\n",
      "    tf.global_variables_initializer().run()\n",
      "    \n",
      "#     saver = tf.train.Saver()\n",
      "#     saver.restore(session, \"./Models/bidirectionaldecoder.ckpt\")\n",
      "    \n",
      "    for step in range(num_steps):\n",
      "        \n",
      "        l2 = 0\n",
      "        for i in range(len(test_mfcc)):\n",
      "            \n",
      "            feedDict = {inputs : test_mfcc[i].reshape([max_encoder_input_size, batch_size, input_size]),\n",
      "                        tar : test_ts[i].reshape([max_decoder_input_size, batch_size, num_classes])}\n",
      "            \n",
      "            \n",
      "            pred = session.run(predictions, feed_dict=feedDict)\n",
      "            \n",
      "            l = 100.0 * np.sum(pred == np.argmax(test_ts[i], 1)) / len(test_ts[i])\n",
      "            \n",
      "            l2 += l\n",
      "            if i % 500 == 0:\n",
      "                print(\"I - test\", i, l)\n",
      "                    \n",
      "        l2 /= len(test_mfcc)\n",
      "        print(\"Test Loss\", l2)\n",
      "        \n",
      "#         l2 = 0\n",
      "#         for i in range(len(train_mfcc)):\n",
      "            \n",
      "#             feedDict = {inputs : train_mfcc[i].reshape([max_encoder_input_size, batch_size, input_size]),\n",
      "#                         tar : train_ts[i].reshape([max_decoder_input_size, batch_size, num_classes])}\n",
      "            \n",
      "            \n",
      "#             _, l = session.run([optimizer, loss], feed_dict=feedDict)\n",
      "            \n",
      "#             l2 += l\n",
      "            \n",
      "#             if i % 500 == 0:\n",
      "#                 print(\"I\", i, l2)\n",
      "                    \n",
      "#         print(\"Step\", step, \"Loss\", l2)\n",
      "#         saver.save(session, \"./Models/bidirectionaldecoder.ckpt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initializing\n",
        "('I - test', 0, 0.0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('I - test', 500, 0.0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('I - test', 1000, 0.0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('I - test', 1500, 0.0)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Test Loss', 2.901360544217692)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}