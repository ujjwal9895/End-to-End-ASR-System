{
 "metadata": {
  "name": "",
  "signature": "sha256:a61efa15656b903c33f9a30b9cf8a2aaeb750c9d0ee060233376aa03aab79176"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tensorflow as tf\n",
      "import pickle\n",
      "import numpy as np\n",
      "from tensorflow.contrib.legacy_seq2seq.python.ops import seq2seq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_mfcc = []\n",
      "train_words = []\n",
      "test_mfcc = []\n",
      "test_words = []\n",
      "vocab_id_2_c = {}\n",
      "vocab_c_2_id = {}\n",
      "x = \"PGabcdefghijklmnopqrstuvwxyz0123456789 ,.'?-UE\"\n",
      "\n",
      "with open(\"train_mfcc\", \"rb\") as f:\n",
      "    train_mfcc = pickle.load(f)\n",
      "    \n",
      "with open(\"train_output_words\", \"rb\") as f:\n",
      "    train_words = pickle.load(f)\n",
      "    \n",
      "with open(\"test_mfcc\", \"rb\") as f:\n",
      "    test_mfcc = pickle.load(f)\n",
      "    \n",
      "with open(\"test_output_words\", \"rb\") as f:\n",
      "    test_words = pickle.load(f)\n",
      "    \n",
      "for i in range(len(x)):\n",
      "    vocab_c_2_id[x[i]] = i + 1\n",
      "    vocab_id_2_c[i + 1] = x[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_hidden = 128\n",
      "num_layers = 2\n",
      "num_classes = len(vocab_c_2_id)\n",
      "batch_size = 1\n",
      "input_size = 123\n",
      "max_encoder_input_size = 0\n",
      "max_decoder_input_size = 0\n",
      "train_ids = []\n",
      "test_ids = []\n",
      "\n",
      "for i in range(len(train_mfcc)):\n",
      "    max_encoder_input_size = max(max_encoder_input_size, len(train_mfcc[i]))\n",
      "    \n",
      "for i in range(len(train_words)):\n",
      "    train_words[i] = train_words[i].lower()\n",
      "    max_decoder_input_size = max(max_decoder_input_size, len(train_words[i]))\n",
      "    \n",
      "for i in range(len(test_words)):\n",
      "    test_words[i] = test_words[i].lower()\n",
      "    \n",
      "for i in range(len(train_mfcc)):\n",
      "    while train_mfcc[i].shape[0] < max_encoder_input_size:\n",
      "        train_mfcc[i] = np.vstack([train_mfcc[i], [0] * 123])\n",
      "        \n",
      "for i in range(len(train_words)):\n",
      "    data = np.zeros((max_decoder_input_size))\n",
      "    data2 = np.zeros((max_decoder_input_size))\n",
      "    for j in range(max_decoder_input_size):\n",
      "        \n",
      "        if j < len(train_words[i]):\n",
      "            if train_words[i][j] in vocab_c_2_id:\n",
      "                data[j] = vocab_c_2_id[train_words[i][j]]\n",
      "            else:\n",
      "                data[j] = vocab_c_2_id['U']\n",
      "        else:\n",
      "            data[j] = 0\n",
      "            \n",
      "        if i < len(test_words) and j < len(test_words[i]):\n",
      "            if test_words[i][j] in vocab_c_2_id:\n",
      "                data2[j] = vocab_c_2_id[test_words[i][j]]\n",
      "            else:\n",
      "                data2[j] = vocab_c_2_id['U']\n",
      "        elif i < len(test_words):\n",
      "            data2[j] = 0\n",
      "    train_ids.append(data)\n",
      "    \n",
      "    if i < len(test_words):\n",
      "        test_ids.append(data2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "graph = tf.Graph()\n",
      "with graph.as_default():\n",
      "    # e.g: log filter bank or MFCC features\n",
      "    # Has size [batch_size, max_stepsize, num_features], but the\n",
      "    # batch_size and max_stepsize can vary along each step\n",
      "    inputs = tf.placeholder(tf.float32, [None, None, input_size])\n",
      "\n",
      "    # Here we use sparse_placeholder that will generate a\n",
      "    # SparseTensor required by ctc_loss op.\n",
      "    targets = tf.sparse_placeholder(tf.int32)\n",
      "    print(targets.get_shape())\n",
      "\n",
      "    # 1d array of size [batch_size]\n",
      "    seq_len = tf.placeholder(tf.int32, [None])\n",
      "\n",
      "    # Defining the cell\n",
      "    # Can be:\n",
      "    #   tf.nn.rnn_cell.RNNCell\n",
      "    #   tf.nn.rnn_cell.GRUCell\n",
      "    cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=True)\n",
      "\n",
      "    # Stacking rnn cells\n",
      "    stack = tf.contrib.rnn.MultiRNNCell([cell] * num_layers,\n",
      "                                        state_is_tuple=True)\n",
      "\n",
      "    # The second output is the last state and we will no use that\n",
      "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
      "\n",
      "    shape = tf.shape(inputs)\n",
      "    batch_s, max_timesteps = shape[0], shape[1]\n",
      "\n",
      "    # Reshaping to apply the same weights over the timesteps\n",
      "    outputs = tf.reshape(outputs, [-1, num_hidden])\n",
      "\n",
      "    # Truncated normal with mean 0 and stdev=0.1\n",
      "    # Tip: Try another initialization\n",
      "    # see https://www.tensorflow.org/versions/r0.9/api_docs/python/contrib.layers.html#initializers\n",
      "    W = tf.Variable(tf.truncated_normal([num_hidden,\n",
      "                                         num_classes],\n",
      "                                        stddev=0.1))\n",
      "    # Zero initialization\n",
      "    # Tip: Is tf.zeros_initializer the same?\n",
      "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
      "\n",
      "    # Doing the affine projection\n",
      "    logits = tf.matmul(outputs, W) + b\n",
      "\n",
      "    # Reshaping back to the original shape\n",
      "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
      "\n",
      "    # Time major\n",
      "    logits = tf.transpose(logits, (1, 0, 2))\n",
      "\n",
      "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
      "    cost = tf.reduce_mean(loss)\n",
      "\n",
      "    optimizer = tf.train.MomentumOptimizer(0.01,\n",
      "                                           0.9).minimize(cost)\n",
      "\n",
      "    # Option 2: tf.contrib.ctc.ctc_beam_search_decoder\n",
      "    # (it's slower but you'll get better results)\n",
      "    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
      "\n",
      "    # Inaccuracy: label error rate\n",
      "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
      "                                          targets))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<unknown>\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_epochs = 100\n",
      "num_examples = len(train_mfcc)\n",
      "\n",
      "with tf.Session(graph=graph) as session:\n",
      "    # Initializate the weights and biases\n",
      "    tf.global_variables_initializer().run()\n",
      "\n",
      "\n",
      "    for curr_epoch in range(num_epochs):\n",
      "        train_cost = train_ler = 0\n",
      "\n",
      "        for i in range(len(train_mfcc)):\n",
      "            \n",
      "            train_id = train_ids[i].reshape([1, max_decoder_input_size])\n",
      "\n",
      "            feed = {inputs: train_mfcc[i].reshape([1, max_encoder_input_size, input_size]),\n",
      "                    targets: train_id,\n",
      "                    seq_len: [max_encoder_input_size]}\n",
      "            \n",
      "            batch_cost, _ = session.run([cost, optimizer], feed)\n",
      "            train_cost += batch_cost*batch_size\n",
      "            train_ler += session.run(ler, feed_dict=feed)*batch_size\n",
      "\n",
      "        train_cost /= num_examples\n",
      "        train_ler /= num_examples\n",
      "\n",
      "        val_feed = {inputs: test_mfcc,\n",
      "                    targets: test_ids,\n",
      "                    seq_len: [max_encoder_input_size]}\n",
      "\n",
      "        val_cost, val_ler = session.run([cost, ler], feed_dict=val_feed)\n",
      "\n",
      "        log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, val_cost = {:.3f}, val_ler = {:.3f}, time = {:.3f}\"\n",
      "        print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler,\n",
      "                         val_cost, val_ler, time.time() - start))\n",
      "    # Decoding\n",
      "    d = session.run(decoded[0], feed_dict=feed)\n",
      "    str_decoded = ''.join([chr(x) for x in np.asarray(d[1]) + FIRST_INDEX])\n",
      "    # Replacing blank label to none\n",
      "    str_decoded = str_decoded.replace(chr(ord('z') + 1), '')\n",
      "    # Replacing space label to space\n",
      "    str_decoded = str_decoded.replace(chr(ord('a') - 1), ' ')\n",
      "\n",
      "    print('Original:\\n%s' % original)\n",
      "    print('Decoded:\\n%s' % str_decoded)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Cannot feed value of shape (80,) for Tensor u'Placeholder_3:0', which has shape '(?, ?)'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-a31d9af95bc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m                     seq_len: [max_encoder_input_size]}\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mbatch_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mtrain_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_cost\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtrain_ler\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    945\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (80,) for Tensor u'Placeholder_3:0', which has shape '(?, ?)'"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}