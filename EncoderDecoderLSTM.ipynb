{
 "metadata": {
  "name": "",
  "signature": "sha256:a61b74eb235045b2489617d134d44eaf50d274eff7d48863a276fa26734ea2b1"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import tensorflow as tf\n",
      "import pickle\n",
      "import numpy as np\n",
      "from tensorflow.contrib.legacy_seq2seq.python.ops import seq2seq"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_mfcc = []\n",
      "train_words = []\n",
      "test_mfcc = []\n",
      "test_words = []\n",
      "vocab_id_2_c = {}\n",
      "vocab_c_2_id = {}\n",
      "x = \"PGabcdefghijklmnopqrstuvwxyz0123456789 ,.'?-UE\"\n",
      "\n",
      "with open(\"train_mfcc\", \"rb\") as f:\n",
      "    train_mfcc = pickle.load(f)\n",
      "    \n",
      "with open(\"train_output_words\", \"rb\") as f:\n",
      "    train_words = pickle.load(f)\n",
      "    \n",
      "with open(\"test_mfcc\", \"rb\") as f:\n",
      "    test_mfcc = pickle.load(f)\n",
      "    \n",
      "with open(\"test_output_words\", \"rb\") as f:\n",
      "    test_words = pickle.load(f)\n",
      "    \n",
      "for i in range(len(x)):\n",
      "    vocab_c_2_id[x[i]] = i + 1\n",
      "    vocab_id_2_c[i + 1] = x[i]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_hidden = 128\n",
      "num_classes = len(vocab_c_2_id)\n",
      "batch_size = 1\n",
      "input_size = 123\n",
      "max_encoder_input_size = 0\n",
      "max_decoder_input_size = 0\n",
      "train_ids = []\n",
      "train_targets = []\n",
      "\n",
      "for i in range(len(train_mfcc)):\n",
      "    max_encoder_input_size = max(max_encoder_input_size, len(train_mfcc[i]))\n",
      "    \n",
      "for i in range(len(train_words)):\n",
      "    train_words[i] = train_words[i].lower()\n",
      "    max_decoder_input_size = max(max_decoder_input_size, len(train_words[i]))\n",
      "    \n",
      "for i in range(len(train_mfcc)):\n",
      "    while train_mfcc[i].shape[0] < max_encoder_input_size:\n",
      "        train_mfcc[i] = np.vstack([train_mfcc[i], [0] * 123])\n",
      "        \n",
      "for i in range(len(train_words)):\n",
      "    data = []\n",
      "    data2 = []\n",
      "    for j in range(max_decoder_input_size):\n",
      "        if j == 0:\n",
      "            data = (np.arange(num_classes) == vocab_c_2_id['G']).astype(np.float32)\n",
      "            if train_words[i][j] in vocab_c_2_id:\n",
      "                data2 = (np.arange(num_classes) == vocab_c_2_id[train_words[i][j]]).astype(np.float32)\n",
      "            else:\n",
      "                data2 = (np.arange(num_classes) == vocab_c_2_id['U']).astype(np.float32)\n",
      "        elif j < len(train_words[i]):\n",
      "            if train_words[i][j-1] in vocab_c_2_id:\n",
      "                data = np.vstack([data, (np.arange(num_classes) == vocab_c_2_id[train_words[i][j-1]]).astype(np.float32)])\n",
      "            else:\n",
      "                data = np.vstack([data, (np.arange(num_classes) == vocab_c_2_id['U']).astype(np.float32)])\n",
      "            if train_words[i][j] in vocab_c_2_id:\n",
      "                data2 = np.vstack([data2, (np.arange(num_classes) == vocab_c_2_id[train_words[i][j]]).astype(np.float32)])\n",
      "            else:\n",
      "                data2 = np.vstack([data2, (np.arange(num_classes) == vocab_c_2_id['U']).astype(np.float32)])\n",
      "        elif j == len(train_words[i]):\n",
      "            if train_words[i][j-1] in vocab_c_2_id:\n",
      "                data = np.vstack([data, (np.arange(num_classes) == vocab_c_2_id[train_words[i][j-1]]).astype(np.float32)])\n",
      "            else:\n",
      "                data = np.vstack([data, (np.arange(num_classes) == vocab_c_2_id['U']).astype(np.float32)])\n",
      "            data2 = np.vstack([data2, [0] * num_classes]) \n",
      "        else:\n",
      "            data = np.vstack([data, [0] * num_classes])\n",
      "            data2 = np.vstack([data2, [0] * num_classes])\n",
      "    train_ids.append(data)\n",
      "    train_targets.append(data2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# encoder_inputs = []\n",
      "# decoder_inputs = []\n",
      "# tar = []\n",
      "\n",
      "# for i in xrange(max_encoder_input_size):\n",
      "#     encoder_inputs.append(tf.placeholder(tf.float32, shape = [None, input_size], name = \"encoder{0}\".format(i)))\n",
      "    \n",
      "# for i in xrange(max_decoder_input_size):\n",
      "#     decoder_inputs.append(tf.placeholder(tf.float32, shape = [None, num_classes], name = \"decoder{0}\".format(i)))\n",
      "#     tar.append(tf.placeholder(tf.float32, shape = [num_classes]))\n",
      "    \n",
      "encoder_inputs = tf.placeholder(tf.float32, shape=(max_encoder_input_size, batch_size, input_size))\n",
      "decoder_inputs = tf.placeholder(tf.float32, shape=(max_decoder_input_size, batch_size, num_classes))\n",
      "\n",
      "encoder_i = []\n",
      "for j in range(max_encoder_input_size):\n",
      "    encoder_i.append(encoder_inputs[j])\n",
      "    \n",
      "decoder_i = []\n",
      "for j in range(max_decoder_input_size):\n",
      "    decoder_i.append(decoder_inputs[j])\n",
      "\n",
      "\n",
      "# encoder_i = tf.split(axis = 0, num_or_size_splits = max_encoder_input_size, value = encoder_i)\n",
      "# decoder_i = tf.split(axis = 0, num_or_size_splits = max_decoder_input_size, value = decoder_i)\n",
      "\n",
      "cell = tf.contrib.rnn.LSTMCell(num_hidden)\n",
      "\n",
      "outputs, state = seq2seq.basic_rnn_seq2seq(encoder_i, decoder_i, cell)\n",
      "\n",
      "weights = tf.get_variable(\"weights1\", [num_hidden, num_classes], dtype = tf.float32)\n",
      "biases = tf.get_variable(\"biases1\", [num_classes], dtype = tf.float32)\n",
      "\n",
      "outs = tf.reshape(outputs, [max_decoder_input_size, num_hidden])\n",
      "logits = tf.matmul(outs, weights) + biases\n",
      "\n",
      "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = decoder_inputs))\n",
      "\n",
      "optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(cost)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_steps = 1000\n",
      "\n",
      "with tf.Session() as session:\n",
      "    \n",
      "    init = tf.global_variables_initializer()\n",
      "    session.run(init)\n",
      "    for step in range(num_steps):\n",
      "        \n",
      "        co = 0\n",
      "        for i in range(len(train_mfcc)):\n",
      "            \n",
      "            inputs = train_mfcc[i].reshape([max_encoder_input_size, batch_size, input_size])\n",
      "#             inputs = []\n",
      "#             for j in range(max_encoder_input_size):\n",
      "#                 inputs.append(train_mfcc[i][j].reshape([batch_size, input_size]))\n",
      "            \n",
      "            outputs = train_ids[i].reshape([max_decoder_input_size, batch_size, num_classes])\n",
      "#             outputs = []\n",
      "#             for j in range(max_decoder_input_size):\n",
      "#                 outputs.append(train_ids[i][j].reshape([batch_size, num_classes])\n",
      "#             t = train_targets[i : i + 5]\n",
      "            \n",
      "#             x_list = {key: value for (key, value) in zip(encoder_inputs, inputs)}\n",
      "#             y_list = {key: value for (key, value) in zip(decoder_inputs, outputs)}\n",
      "#             z_list = {key: value for (key, value) in zip(tar, t)}\n",
      "\n",
      "#             _, c, _ = session.run([optimizer, cost, outputs],\n",
      "#                                   feed_dict = dict(x_list.items() + y_list.items()))\n",
      "            \n",
      "            _, c = session.run([optimizer, cost],\n",
      "                                  feed_dict = {encoder_inputs : inputs, decoder_inputs : outputs})\n",
      "            co += c\n",
      "            if i % 1000 == 0:\n",
      "                print(co)\n",
      "        print(\"Step\", step, \"Loss\", co)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.24401974678\n",
        "1953.71351892"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3859.94096911"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5769.96374208"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7677.27312773"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Step', 0, 'Loss', 8866.9248358607292)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1.81056022644"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1914.23610479"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3823.32901746"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5735.24725908"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7640.89484167"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Step', 1, 'Loss', 8818.5545116066933)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1.81135237217"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-018c4112d207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             _, c = session.run([optimizer, cost],\n\u001b[0;32m---> 31\u001b[0;31m                                   feed_dict = {encoder_inputs : inputs, decoder_inputs : outputs})\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mco\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}